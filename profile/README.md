# [AI Evals for Engineers & PMs](bit.ly/evals-ai)

**Learn practical, data-driven methods to quickly evaluate and improve AI applications.**

---

### ü§î Do you catch yourself asking any of the following questions while building AI applications?

1. How do I test applications when the outputs are stochastic and require subjective judgements?
2. If I change the prompt, how do I know I'm not breaking something else?
3. Where should I focus my engineering efforts? Do I need to test everything?
4. What if I have no data or customers, where do I start?
5. What metrics should I track? What tools should I use? Which models are best?
6. Can I automate testing and evaluation? If so, how do I trust it?

**If you aren't sure about the answers to these questions, this course is for you.**

### üõ†Ô∏è What you'll learn

1. Fundamentals & Lifecycle of LLM Evaluation
2. Systematic Error Analysis
3. Implementing Effective Evaluations
4. Collaborative Evaluation Practices
5. Architecture-Specific Strategies
6. Production Monitoring & Continuous Evaluation
7. Efficient Continuous Human Review Systems
8. Cost Optimization Techniques


### üë• Who should attend?

* Engineers & technical PMs building AI products.
* Developers seeking rigorous evaluation beyond basic prompt tuning.
* Teams aiming to automate and trust their AI testing.

---
[Link to course](bit.ly/evals-ai)
